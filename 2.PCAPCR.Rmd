---
title: "IODS-Final"
author: "Tuure Parviainen"
date: "December 13, 2017"
output: html_document
---
# IODS-Final: Boston dataset PCA and PCR


We tried linear regression on the Boston dataset and we found that we couldn't reduce the dimensionality much even with the transforming the dataset. So now we will explore the dataset with PCA and then try to use the dimensions for making predictions. wE

1. Normal unchanged dataset
    + 1a Normal train
    + 1b Normal test


```{r, message=FALSE, warning=FALSE}

knitr::opts_chunk$set(echo = TRUE, warning=FALSE)
require("easypackages") # for loading and installing many packages at one time
packages_to_load <- c("broom", "dplyr", "tidyverse", "corrplot", "ggplot2", "GGally", "caret", "devtools", "ggthemes", "scales", "reshape2", "FactoMineR", "factoextra", "pls", "visreg", "mgcv", "WVPlots")
packages(packages_to_load, prompt = TRUE) # lock'n'load install/load

```


```{r, message=FALSE, warning=FALSE}

# set the seed to make your partition reproductible
set.seed(777)

# 1) Normal unchanged dataset
boston <- read.table("data/boston.txt")

# 70% of the sample size
sample_size <- floor(0.70 * nrow(boston))

train_ind <- sample(seq_len(nrow(boston)), size = sample_size)
# splitting data by the vector
trainb <- boston[train_ind, ]
testb <- boston[-train_ind, ]

```



```{r, message=FALSE, warning=FALSE}
######################################################################################
# PCA

# Note that, by default, the function PCA() [in FactoMineR], standardizes the data automatically during the PCA; so you donâ€™t need do this transformation before the PCA.

res.pca <- PCA(boston, graph = FALSE)
#summary(res.pca)

# How many dimensions we need
fviz_eig(res.pca, addlabels = TRUE, ylim = c(0, 50))
```

First we look at the scree plot to see if we are able to reduce dimensions. The first dimension seems to explain 46.8%, but in order to explain more than 90 %, we need more than 8 dimensions. It seems that it might be difficult to reduce dimensions compared to linear regression.

```{r, message=FALSE, warning=FALSE}

# Individuals and dimensions
fviz_pca_biplot(res.pca, label = "var")
```

We can see from the figure that in the 1st dimension the first positive correlated variables are:  proportion of non-retail business acres per town, index of accessibility to radial highways, proportion of owner-occupied units built prior to 1940.


```{r, message=FALSE, warning=FALSE}
# Contributions of variables to PC1, PC2, PC3
# fviz_contrib(res.pca, choice = "var", axes = 1, top = 10)
#fviz_contrib(res.pca, choice = "var", axes = 2, top = 10)
#fviz_contrib(res.pca, choice = "var", axes = 3, top = 10)
#fviz_contrib(res.pca, choice = "var", axes = 1:3, top = 10)

#eig.val <- get_eigenvalue(res.pca)
#eig.val

# With eigenvalue higher than 1 are included. dim 1:3.

# Most influential variables.
fviz_pca_var(res.pca, select.var= list(cos2 = 5))

# With LM we had: crim + indus + age + dis + rad + ptratio + medv
# With PCA:  medv + indus + dis + lstat and nox
# The trouble is our data is still highly dimensional. What to do?

```

Here we can see the five most influential variables. These are medv: median value of owner-occupied homes divided by $1000s, nox:  nitrogen oxides concentration (parts per 10 million). indus: proportion of non-retail business acres per town. lstat: lower status of the population (percent). and dis: weighted mean of distances to five Boston employment centres.

Next we are going to use the Principal component regression using pls-library.

```{r, message=FALSE, warning=FALSE}
######################################################################################
# LM on PCA so PCR - transformed dataset

pcr_model <- pcr(nox~., data = boston, scale = TRUE, validation = "CV")
#summary(pcr_model)

#validationplot(pcr_model)
#validationplot(pcr_model, val.type="MSEP")
validationplot(pcr_model, val.type = "R2")

# What you would like to see is a low cross validation error with a lower number of components than the number of variables in your dataset. If this is not the case or if the smalles cross validation error occurs with a number of components close to the number of variables in the original data, then no dimensionality reduction occurs. In the example above, it looks like 3 components are enough to explain more than 90% of the variability in the data although the CV score is a little higher than with 4 or 5 components. Finally, note that 6 components explain all the variability as expected.
#predplot(pcr_model)
#coefplot(pcr_model)

#Test train
pcr_model <- pcr(nox~., data = trainb ,scale =TRUE, validation = "CV")

pcr_pred <- predict(pcr_model, testb, ncomp = 6)

# MSE
mean((pcr_pred - testb$nox)^2)

# 0.003793769
print("PCR seems slightly worse than linear regression.")

#plot(pcr_model, ncomp = 6) # Plot of cross-validated predictions
#plot(pcr_model, "scores") # Score plot
#plot(pcr_model, "loadings", comps = 1:3) # The three first loadings
#plot(pcr_model, "coef", ncomp = 5) # Coefficients
#plot(pcr_model, "val") # RMSEP curves
plot(pcr_model, "val", val.type = "MSEP", estimate = "CV") # CV MSEP

pcr_pred <- data.frame(pcr_pred)
pcr.scatter<- cbind(pcr_pred, testb$nox)
colnames(pcr.scatter) <- c("pcr_pred","nox")

ScatterHist(pcr.scatter, "pcr_pred", "nox",
            smoothmethod="identity",
            annot_size=3,
            title="Recovered model versus truth (Normal dataset, Pricipal component regression)")

```

We were able to reduce the dimensions of the data to **6**, however we also reduced the R^2 to **72%** and with MSE (mean squared error) of **0.003667294**. The dimensionality reduction is similar than what can be achived by scaling and transforming the dataset. We can however achive even dimensionality reduction to **2** dimensions with R^2 of **68%**.


